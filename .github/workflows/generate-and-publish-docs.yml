name: Generate & Publish Docs

on:
  push:
    branches: [main]
    paths-ignore:
      - ".ai/docs/**"
      - ".ai/review-log/**"
      - ".github/workflows/generate-and-publish-docs.yml"

permissions:
  contents: write

env:
  # Regenerate unconditionally if docs are older than this many days
  DOCS_MAX_AGE_DAYS: 14

jobs:
  generate-and-publish:
    name: Generate and publish docs
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # full history needed for git log age checks

      - name: Check whether docs need regenerating
        id: freshness
        run: |
          DOCS_COMMIT=$(git log -1 --format="%H" -- .ai/docs/ 2>/dev/null || true)

          if [[ -z "$DOCS_COMMIT" ]]; then
            echo "reason=no docs exist" >> "$GITHUB_OUTPUT"
            echo "generate=true"       >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Source file patterns that affect doc content
          CHANGED=$(git diff --name-only "${DOCS_COMMIT}..HEAD" -- \
            '*.tf' '*.py' '*.sh' '*.md' 'Dockerfile*' 'docker-compose*' \
            | grep -v '^\.ai/' || true)

          if [[ -n "$CHANGED" ]]; then
            echo "reason=source files changed since last generation" >> "$GITHUB_OUTPUT"
            echo "generate=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          DOCS_TS=$(git log -1 --format="%ct" -- .ai/docs/)
          AGE_DAYS=$(( ( $(date +%s) - DOCS_TS ) / 86400 ))

          if (( AGE_DAYS > DOCS_MAX_AGE_DAYS )); then
            echo "reason=docs are ${AGE_DAYS} days old (max: ${DOCS_MAX_AGE_DAYS})" >> "$GITHUB_OUTPUT"
            echo "generate=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "reason=docs are current (${AGE_DAYS}d old, no source changes)" >> "$GITHUB_OUTPUT"
          echo "generate=false" >> "$GITHUB_OUTPUT"

      - name: Report freshness decision
        run: |
          if [[ "${{ steps.freshness.outputs.generate }}" == "true" ]]; then
            echo "::notice::Regenerating docs — ${{ steps.freshness.outputs.reason }}"
          else
            echo "::notice::Skipping doc generation — ${{ steps.freshness.outputs.reason }}"
          fi

      - uses: actions/setup-node@v4
        if: steps.freshness.outputs.generate == 'true'
        with:
          node-version: "20"

      - name: Cache Gemini CLI
        if: steps.freshness.outputs.generate == 'true'
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: gemini-cli-${{ runner.os }}

      - name: Install Gemini CLI
        if: steps.freshness.outputs.generate == 'true'
        run: npm install -g @google/gemini-cli

      - name: Generate docs
        if: steps.freshness.outputs.generate == 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          mkdir -p ~/.gemini
          bash scripts/generate-docs.sh all

      - name: Commit generated docs
        if: steps.freshness.outputs.generate == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .ai/docs/
          if ! git diff --cached --quiet; then
            git commit -m "docs: auto-generate [skip ci]"
            git push
          else
            echo "Docs unchanged — nothing to commit."
          fi

      - name: Publish to Wiki.js
        if: steps.freshness.outputs.generate == 'true'
        env:
          WIKIJS_URL: ${{ secrets.WIKIJS_URL }}
          WIKIJS_API_KEY: ${{ secrets.WIKIJS_API_KEY }}
        run: |
          python3 << 'PYEOF'
          import json, os, sys, urllib.request
          from pathlib import Path

          docs_dir  = Path(".ai/docs")
          repo_name = "${{ github.repository }}".split("/")[-1]
          repo_url  = "https://github.com/${{ github.repository }}"

          DOC_MAP = {
              "architecture":        ("GitHub-Projects/{repo}/architecture",       "Architecture Overview"),
              "features":            ("GitHub-Projects/{repo}/features",           "Feature List"),
              "developer-guide":     ("GitHub-Projects/{repo}/developer-guide",    "Developer Guide"),
              "support-guide":       ("GitHub-Projects/{repo}/support-guide",      "Support Guide"),
              "testing":             ("GitHub-Projects/{repo}/testing",            "Test Procedures"),
              "bug-list":            ("GitHub-Projects/{repo}/bug-list",           "Bug List & Recommendations"),
              "performance":         ("GitHub-Projects/{repo}/performance",        "Performance Improvements"),
              "ai-development-plan": ("GitHub-Projects/{repo}/ai-development-plan","AI Development Plan"),
          }

          gql_url = os.environ["WIKIJS_URL"] + "/graphql"
          headers = {
              "Authorization": f"Bearer {os.environ['WIKIJS_API_KEY']}",
              "Content-Type": "application/json",
          }

          def graphql(query, variables=None):
              payload = json.dumps({"query": query, "variables": variables or {}}).encode()
              req = urllib.request.Request(gql_url, data=payload, headers=headers)
              try:
                  with urllib.request.urlopen(req) as resp:
                      return json.loads(resp.read())
              except urllib.error.HTTPError as e:
                  print(f"::error::Wiki.js API error {e.code}: {e.read().decode()}")
                  return None

          result   = graphql("{ pages { list(orderBy: TITLE) { id, path } } }")
          pages    = result.get("data", {}).get("pages", {}).get("list", [])
          page_map = {p["path"]: p["id"] for p in pages}

          def upsert(page_path, title, content, tags):
              if page_path in page_map:
                  r = graphql(
                      """mutation ($id: Int!, $content: String!, $tags: [String]!) {
                          pages { update(id: $id, content: $content, isPublished: true, tags: $tags) {
                              responseResult { succeeded, message }
                          }}
                      }""",
                      {"id": page_map[page_path], "content": content, "tags": tags},
                  )
                  resp = r["data"]["pages"]["update"]["responseResult"]
              else:
                  r = graphql(
                      """mutation ($content: String!, $path: String!, $title: String!) {
                          pages { create(
                              content: $content, description: "", editor: "markdown",
                              isPublished: true, isPrivate: false, locale: "en",
                              path: $path, tags: ["generated", "ai-docs"], title: $title
                          ) { responseResult { succeeded, message } page { id } } }
                      }""",
                      {"content": content, "path": page_path, "title": title},
                  )
                  resp = r["data"]["pages"]["create"]["responseResult"]
              print(f"  {'✓' if resp['succeeded'] else '✗'} {resp['message']}")

          # Publish each doc page
          published = []
          for doc_file in sorted(docs_dir.glob("*.md")):
              stem = doc_file.stem
              if stem not in DOC_MAP:
                  print(f"Skipping unmapped doc: {stem}")
                  continue
              path_tpl, title = DOC_MAP[stem]
              page_path = path_tpl.format(repo=repo_name)
              content   = f"> *Auto-generated — do not edit manually. [Source]({repo_url})*\n\n"
              content  += doc_file.read_text()
              print(f"Publishing: {title} → /{page_path}")
              upsert(page_path, title, content, ["generated", "ai-docs"])
              published.append((page_path, title))

          # Publish repo index page so the parent path isn't a 404
          index_path = f"GitHub-Projects/{repo_name}"
          index_lines = [
              f"# {repo_name}",
              f"",
              f"> Auto-generated documentation for [{repo_name}]({repo_url}).",
              f"",
              f"## Documents",
              f"",
          ]
          for page_path, title in sorted(published, key=lambda x: x[1]):
              # Wiki.js internal links use the path relative to the root
              index_lines.append(f"- [{title}](/{page_path})")
          index_lines += [
              f"",
              f"---",
              f"",
              f"*Generated from [{repo_url}]({repo_url}) — do not edit manually.*",
          ]
          print(f"Publishing: index → /{index_path}")
          upsert(index_path, repo_name, "\n".join(index_lines), ["generated", "ai-docs"])
          PYEOF
